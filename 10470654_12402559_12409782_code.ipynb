{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- INSERT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from typing import List\n",
    "from functools import lru_cache\n",
    "from copy import deepcopy\n",
    "from random import choice, random\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "import scipy.stats\n",
    "import cProfile, pstats, io\n",
    "import os\n",
    "\n",
    "\n",
    "# ### DEBUG MODE\n",
    "# # to add debugging, open terminal and type 'tail -1000f debug.txt'. Also uncomment next line\n",
    "# debug = open(\"debug.txt\", \"w\")\n",
    "# # you can then write to terminal with the following python command: debug.write(\"something \\n\")\n",
    "\n",
    "\n",
    "### GLOBAL CONSTANTS\n",
    "\n",
    "# Caching\n",
    "CACHING_ON = False\n",
    "\n",
    "# Number of docs created\n",
    "DATABASE_SIZE = 6\n",
    "\n",
    "# Gamma's get trained on this many docs:\n",
    "GAMMA_SIZE = 3 # also defines cutoff\n",
    "\n",
    "\n",
    "### UTILS\n",
    "\n",
    "def softmax(distribution):\n",
    "    \"\"\" used to restore probability constraint of summing to 1 when elements are removed \"\"\"\n",
    "\n",
    "    summation = sum(distribution)\n",
    "    return [float(x / summation) for x in distribution]\n",
    "\n",
    "@lru_cache(maxsize=3200)\n",
    "def difference_to_err_table_position(difference: int) -> int:\n",
    "    # if difference < 0.05 or \n",
    "    if difference > 0.95:\n",
    "        raise Exception(\"Invalid difference\")\n",
    "    elif difference < 0.1:\n",
    "        return 0\n",
    "    elif difference < 0.2:\n",
    "        return 1\n",
    "    elif difference < 0.3:\n",
    "        return 2\n",
    "    elif difference < 0.4:\n",
    "        return 3\n",
    "    elif difference < 0.5:\n",
    "        return 4\n",
    "    elif difference < 0.6:\n",
    "        return 5\n",
    "    elif difference < 0.7:\n",
    "        return 6\n",
    "    elif difference < 0.8:\n",
    "        return 7\n",
    "    elif difference < 0.9:\n",
    "        return 8\n",
    "    else:\n",
    "        return 9\n",
    "\n",
    "def initialize_err_table():\n",
    "    err_table = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: [],\n",
    "        6: [],\n",
    "        7: [],\n",
    "        8: [],\n",
    "        9: []\n",
    "    }\n",
    "\n",
    "    return err_table\n",
    "\n",
    "# check\n",
    "if (GAMMA_SIZE > 10):\n",
    "    GAMMA_SIZE = 10\n",
    "elif (GAMMA_SIZE < 1):\n",
    "    GAMMA_SIZE = 1\n",
    "    \n",
    "\n",
    "def split_to_chunks(list_to_split, chunks_size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(list_to_split), chunks_size):\n",
    "        chunks.append(list_to_split[i:i + chunks_size])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def average_chunks(list_to_split, max_chunks):\n",
    "    chunks_size = math.ceil(len(list_to_split) / max_chunks)\n",
    "    chunks = split_to_chunks(list_to_split, chunks_size)\n",
    "\n",
    "    result = []\n",
    "    for chunk in chunks:\n",
    "        chunk_average = np.average(chunk)\n",
    "        result.append(chunk_average)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA MANAGEMENT\n",
    "\n",
    "class Saver():\n",
    "\n",
    "    def __init__(self, directory, caching=False):\n",
    "        self.directory = directory\n",
    "        self.caching = caching\n",
    "        self.notified = False\n",
    "\n",
    "    def save_python_obj(self, obj, name):\n",
    "        if (not self.caching):\n",
    "            if (not self.notified):\n",
    "                print(\"WARNING: caching disabled, nothing will be pickled. See boolean CACHING_ON on top of notebook\")\n",
    "                self.notified = True\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with open(self.directory + name+\".pickle\", 'wb') as handle:\n",
    "                pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"Saved {}\".format(name))\n",
    "        except:\n",
    "            print(\"Failed saving {}, continue anyway\".format(name))\n",
    "\n",
    "    def load_python_obj(self, name):\n",
    "        if (not self.caching):\n",
    "            if (not self.notified):\n",
    "                print(\"WARNING: caching disabled, nothing will be pickled. See boolean CACHING_ON on top of notebook\")\n",
    "                self.notified = True\n",
    "            raise FileNotFoundError(\"Caching disabled\")\n",
    "        \n",
    "        obj = None\n",
    "        try:\n",
    "            with (open(self.directory + name+\".pickle\", \"rb\")) as openfile:\n",
    "                obj = pickle.load(openfile)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"{} not loaded because file is missing\".format(name))\n",
    "        print(\"Loaded {}\".format(name))\n",
    "        return obj\n",
    "\n",
    "    def load_data_model_1(self):\n",
    "        try:\n",
    "            frame = self.load_python_obj(\"data_model_1\")\n",
    "            print(\"Loaded sucessfully\")\n",
    "            return frame\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            print(\"Building data framework\")\n",
    "\n",
    "            f = open(self.directory+\"YandexRelPredChallenge.txt\", \"r\")\n",
    "            frame = []\n",
    "            for line in f:\n",
    "\n",
    "                line = line.replace(\"\\n\", \"\")\n",
    "\n",
    "                elements = line.split(\"\t\")\n",
    "\n",
    "                if (elements[2] == \"C\"):\n",
    "                    dictionary = {\"SessionID\": int(elements[0]),\n",
    "                                  \"TimePassed\": int(elements[1]),\n",
    "                                  \"TypeOfAction\": elements[2],\n",
    "                                  \"URLID\": int(elements[3])}\n",
    "                elif (elements[2] == \"Q\"):\n",
    "                    dictionary = {\"SessionID\": int(elements[0]),\n",
    "                                  \"TimePassed\": int(elements[1]),\n",
    "                                  \"TypeOfAction\": elements[2],\n",
    "                                  \"QueryID\": int(elements[3]),\n",
    "                                  \"RegionID\": int(elements[4]),\n",
    "                                  \"ListOfURLs\": [int(x) for x in elements[5:5+GAMMA_SIZE]]\n",
    "\n",
    "                                  }\n",
    "                else:\n",
    "                    raise Exception(\"contenttype not recognized, check load data function\")\n",
    "\n",
    "                frame.append(dictionary)\n",
    "\n",
    "            self.save_python_obj(frame, \"data_model_1\")\n",
    "\n",
    "            print(\"Created data model, this only needs to be done once if caching is on\")\n",
    "            return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General IR-step class\n",
    "used to define the process structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRStep(object):\n",
    "\n",
    "    def __init__(self, name=None, purpose=None, data=None):\n",
    "        self.name = name\n",
    "        self.purpose = purpose\n",
    "        self.data = data\n",
    "\n",
    "    def do_step(self, input_list):\n",
    "        print(\"Starting step {}\".format(self.name))\n",
    "        if (not self.purpose == None):\n",
    "            print(\"Goal:\" + self.purpose + \"\\n\\n\")\n",
    "\n",
    "        result = self.onStart(input_list)\n",
    "        self.onfinish()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "        raise Exception(\"method to be overrided by subclass Step#\")\n",
    "\n",
    "    def onfinish(self):\n",
    "        raise Exception(\"method to be overrided by subclass Step#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 implementation:\n",
    "- Getting the rankings out of permutations\n",
    "- Creating the documents\n",
    "- Pairing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    \n",
    "    def __init__(self, id, relevance : int):\n",
    "        self.id = id\n",
    "        self.relevance = relevance\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({\"relevance\" : self.relevance_to_int(), \"id\": self.id})\n",
    "        \n",
    "    def relevance_to_int(self):\n",
    "        return self.relevance\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingsStep(IRStep):\n",
    "\n",
    "    def __init__(self, name, purpose, data):\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "        print(\"--- generating documents...\")\n",
    "\n",
    "        documents = self.generate_documents(input_list[0])\n",
    "\n",
    "        print(\"--- generating rankings...\")\n",
    "        p_rankings = itertools.permutations(documents, r=3)\n",
    "        e_rankings = itertools.permutations(documents, r=3)\n",
    "        \n",
    "        print(\"--- generating ranking pairs...\")\n",
    "        rankings_pairs = list(itertools.product(p_rankings, e_rankings))\n",
    "        rankings_pairs = [(list(item[0]), list(item[1])) for item in rankings_pairs]\n",
    "\n",
    "        print(f'--- finished generating {len(rankings_pairs)} ranking pairs')\n",
    "                \n",
    "        return rankings_pairs\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"\\n\\nFinished step {}\\n\\n\".format(self.name))\n",
    "\n",
    "    def generate_documents(self, number) -> List[Document]:\n",
    "        current_id = 1\n",
    "        documents = []\n",
    "        \n",
    "        # create NOT_RELEVANT documents \n",
    "        for i in range(number):\n",
    "            documents.append(Document(current_id, 0))\n",
    "            current_id += 1\n",
    "\n",
    "        # create RELEVANT documents\n",
    "        for i in range(number):\n",
    "            documents.append(Document(current_id, 1))\n",
    "            current_id += 1\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 implementation:\n",
    "- Evaluating EER score\n",
    "- Binnify pairs\n",
    "- Filtering not promising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERRStep(IRStep):\n",
    "\n",
    "    def __init__(self, name, purpose, data):\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "    def onStart(self, ranking_pairs):\n",
    "        err_table = initialize_err_table()\n",
    "        \n",
    "        counter = 0\n",
    "        for ranking_pair in ranking_pairs:\n",
    "\n",
    "            try:\n",
    "                p_err = self.calculate_err(ranking_pair[0])\n",
    "                e_err = self.calculate_err(ranking_pair[1])\n",
    "\n",
    "                difference = e_err - p_err\n",
    "\n",
    "                # if E does not outperform P, discard this pair\n",
    "                if difference <= 0:\n",
    "                    continue\n",
    "                counter += 1\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            err_table_position = difference_to_err_table_position(difference)\n",
    "            err_table[err_table_position].append(ranking_pair)\n",
    "\n",
    "#             # DEBUG LINE FOR GETTING LESS DOCS\n",
    "#             if counter >= 100:\n",
    "#                 break\n",
    "\n",
    "        print (f'total ranking pairs left: {counter}')\n",
    "        \n",
    "        return err_table\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"Finished step {}\".format(self.name))\n",
    "\n",
    "    def calculate_err(self, documents: [Document]):\n",
    "        err_score = 0\n",
    "\n",
    "        max_relevance = 0\n",
    "        for document in documents:\n",
    "            document_relevance = document.relevance_to_int()\n",
    "            if document_relevance > max_relevance:\n",
    "                max_relevance = document_relevance\n",
    "\n",
    "        for r, document in enumerate(documents):\n",
    "            inner_result = 1\n",
    "            for i in range(r):\n",
    "                theta_i = (2**(documents[i].relevance_to_int()) - 1)/ (2**(max_relevance))\n",
    "                inner_result *= (1 - theta_i)\n",
    "        \n",
    "            theta_r = (2**(document.relevance_to_int()) - 1)/ (2**(max_relevance))\n",
    "            current_err_score = inner_result * theta_r\n",
    "            current_err_score /= (r + 1)\n",
    "            \n",
    "            err_score += current_err_score\n",
    "        \n",
    "        return err_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 implementation:\n",
    "- Interleaving ranking pairs using the following models:\n",
    "    - Team draft interleaving\n",
    "    - Probabilistic interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Main interleaving class, parent to specific interleavings\n",
    "\n",
    "Holds some main functionality\n",
    "\"\"\"\n",
    "\n",
    "class Interleaving(object):\n",
    "\n",
    "\n",
    "    def __init__(self, alg_P, alg_E, cutoff=None):\n",
    "        self.alg_P = alg_P\n",
    "        self.alg_E = alg_E\n",
    "        self.ranking2algorithm = {0: \"P\", 1: \"E\"}\n",
    "        self.position2ranking = {}\n",
    "        self.interleaved = []\n",
    "        self.score = {\"E\" : 0, \"P\" : 0}\n",
    "        self._interleave_docs()\n",
    "        if (not cutoff == None):\n",
    "            self.cut_off_at(cutoff)\n",
    "        self.registered_clicks = 0\n",
    "        self.click_history = []\n",
    "\n",
    "    def _interleave_docs(self): #PRIVATE\n",
    "        \"\"\" method contracty to be overrided by child-classes \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"To be overrided by child class\")\n",
    "\n",
    "    def insertclick(self, position): \n",
    "        \"\"\" stores a click in the interleaving such that later the score can be extracted \"\"\"\n",
    "\n",
    "        self.score[self.ranking2algorithm[self.position2ranking[position]]] += 1\n",
    "        self.registered_clicks += 1\n",
    "        self.click_history.append(position)\n",
    "\n",
    "    def get_interleaved_ranking(self) -> List[Document]: \n",
    "        \"\"\" returns list of documents \"\"\"\n",
    "\n",
    "        return self.interleaved\n",
    "\n",
    "    def get_click_history(self): \n",
    "        return self.click_history\n",
    "\n",
    "    def get_score(self): \n",
    "        \"\"\" returns the score of the two isnerted rankings given currently registered clicks \"\"\"\n",
    "\n",
    "        return self.score\n",
    "\n",
    "    def reset_score(self): \n",
    "        \"\"\" resets counters but leaves interleaving intact \"\"\"\n",
    "\n",
    "        self.score = {\"E\": 0, \"P\": 0}\n",
    "        self.registered_clicks = 0\n",
    "        self.click_history = []\n",
    "\n",
    "    def get_winner(self): \n",
    "        \"\"\" gets winner of interleaving \"\"\"\n",
    "\n",
    "        if (self.score[\"E\"] == self.score[\"P\"]):\n",
    "            return -1\n",
    "        return max(self.score, key=self.score.get)\n",
    "\n",
    "    def cut_off_at(self, cutoff): \n",
    "        \"\"\" cuts off interleaving after certain rank. note: expectations are not recalculated \"\"\"\n",
    "\n",
    "        self.interleaved = self.interleaved[:cutoff]\n",
    "        for key in list(self.position2ranking):\n",
    "            if (key > cutoff):\n",
    "                del self.position2ranking[key]\n",
    "\n",
    "    def _remove_duplicates_from_other_ranking(self, rankings, picked_document, counters, which_second, distributions=None): #PRIVATE\n",
    "        \"\"\" buisiness logic function for removing duplicates out of the ranking whos turn it is not to add a element to the interleaving \"\"\"\n",
    "\n",
    "        # get doc ids from the other ranking and see at what places the doc occurs\n",
    "        doc_ids_second_player = [doc.id for doc in rankings[which_second]]\n",
    "\n",
    "        if (picked_document.id in doc_ids_second_player):\n",
    "            index = doc_ids_second_player.index( picked_document.id)\n",
    "\n",
    "            removed = rankings[which_second].pop(index)\n",
    "            counters[which_second] -= 1\n",
    "\n",
    "            # make sure the removed objects ar identical\n",
    "            assert removed.id == picked_document.id, \"Mistake in prob-interleaving: removing docs from other ranking\"\n",
    "\n",
    "            return self._pop_distribution(index, distributions, which_second)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _pop_distribution(self, index, distributions, which_second): #PRIVATE\n",
    "        \"\"\" to be overrided by child-classes that utilize it, to be ignored by those who don't \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __str__(self): #TO STRING\n",
    "        return \"Interleaving: \" + str(self.get_interleaved_ranking()) + \", Scores: \" + str(self.score), \", Registered clicks: \"+ str(self.registered_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProbabilisticInterleaving(Interleaving):\n",
    "\n",
    "    def __init__(self, alg_P, alg_E, distribution, cutoff=None):\n",
    "        self.distribution = distribution\n",
    "        assert len(distribution) == len(alg_P) == len(\n",
    "            alg_E), \"rankings and/or distribution not fo the same lengths\"\n",
    "        self.possible_generators = len(alg_P) * len(alg_E)\n",
    "        self.position2chance = {}\n",
    "        super().__init__(alg_P, alg_E, cutoff)\n",
    "\n",
    "\n",
    "    def _interleave_docs(self): #PRIVATE\n",
    "        \"\"\" implementation of interleaving \"\"\"\n",
    "\n",
    "        counters = [len(self.alg_P), len(self.alg_E)]\n",
    "        rankings = deepcopy([self.alg_P, self.alg_E])\n",
    "        distributions = deepcopy([self.distribution])+deepcopy([self.distribution])\n",
    "\n",
    "        while (sum(counters) > 0):\n",
    "\n",
    "            # flip coin\n",
    "            which_first = choice([1, 0])\n",
    "            which_second = int(not which_first)\n",
    "\n",
    "            # take doc from chosen ranking, skip if it is empty\n",
    "            if (counters[which_first] == 0):\n",
    "                continue\n",
    "            counters[which_first] -= 1\n",
    "            picked_index = np.random.choice(range(len(distributions[which_first])), p=softmax(distributions[which_first]), replace=False)\n",
    "            picked_document = rankings[which_first].pop(picked_index)\n",
    "            chance_which_first = self._pop_distribution(picked_index, distributions, which_first)\n",
    "\n",
    "            # remove from other ranking\n",
    "            chance_which_second = self._remove_duplicates_from_other_ranking(rankings, picked_document, counters, which_second, distributions=distributions)\n",
    "\n",
    "            # insert into interleaving\n",
    "            self.position2chance[len(self.interleaved)] = {which_first: chance_which_first, which_second: chance_which_second}\n",
    "            self.interleaved.append(picked_document)\n",
    "\n",
    "        # make sure both rankings are empty\n",
    "        assert len(rankings[0]) + len(rankings[1]) == 0, \"Mistake: not ranking all docs\"\n",
    "\n",
    "        # complete expectation calculation\n",
    "        self._fill_in_expectations()\n",
    "\n",
    "\n",
    "\n",
    "    def insertclick(self, position): # USE THIS\n",
    "        \"\"\" implementation of click-saving \"\"\"\n",
    "\n",
    "        self.score[\"P\"] += self.position2ranking[position][0]\n",
    "        self.score[\"E\"] += self.position2ranking[position][1]\n",
    "\n",
    "        self.registered_clicks += 1\n",
    "        self.click_history.append(position)\n",
    "\n",
    "    def _pop_distribution(self, index, distributions, which): # PRIVATE\n",
    "        \"\"\" removes element form probability distribution as to be consistent with the documents to be interleaved\"\"\"\n",
    "\n",
    "        return distributions[which].pop(index)\n",
    "\n",
    "    def _fill_in_expectations(self): # PRIVATE\n",
    "        \"\"\" pre-calculates the expectation that is added to both players per new future click \"\"\"\n",
    "\n",
    "        # get all permutations that could've generated this interleaving\n",
    "        chance_of_permutations = []\n",
    "        contribution_permutations = list(itertools.product([0, 1], repeat=len(self.interleaved)))\n",
    "\n",
    "        # get the prior chance of that permutation\n",
    "        for permutation in contribution_permutations:\n",
    "            chance_of_permutations.append(float(sum([self.position2chance[i][r] for i, r in zip(range(len(self.interleaved)), permutation)])/(self.possible_generators*0.5)))\n",
    "\n",
    "        # for both rankings, calculate expectated clicks earned for each future click on each position\n",
    "        for position in range(len(self.interleaved)):\n",
    "\n",
    "            expectations = [0,0]\n",
    "\n",
    "            for chance, permutation in zip(chance_of_permutations, contribution_permutations):\n",
    "                expectations[permutation[position]] += self.position2chance[position][permutation[position]]*chance\n",
    "\n",
    "            self.position2ranking[position] = {0: expectations[0], 1 : expectations[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamDraftInterleaving(Interleaving):\n",
    "\n",
    "    def __init__(self, alg_P, alg_E, cutoff=None):\n",
    "        super().__init__(alg_P, alg_E, cutoff)\n",
    "\n",
    "    def _interleave_docs(self): # PRIVATE\n",
    "        \"\"\" implementation of interleaving \"\"\"\n",
    "\n",
    "        counters = [len(self.alg_P), len(self.alg_E)]\n",
    "        rankings = deepcopy([self.alg_P, self.alg_E])\n",
    "\n",
    "        while(sum(counters) > 0):\n",
    "\n",
    "            # flip coin\n",
    "            which_first = choice([1, 0])\n",
    "            which_second = int(not which_first)\n",
    "\n",
    "            # take doc from chosen ranking, skip if it is empty\n",
    "            if (counters[which_first] == 0):\n",
    "                continue\n",
    "            counters[which_first] -= 1\n",
    "            picked_document = rankings[which_first].pop(0)\n",
    "\n",
    "            self._remove_duplicates_from_other_ranking(rankings, picked_document, counters, which_second)\n",
    "\n",
    "            # insert into interleaving\n",
    "            self.position2ranking[len(self.interleaved)] = which_first\n",
    "            self.interleaved.append(picked_document)\n",
    "\n",
    "        # make sure both rankings are empty\n",
    "        assert len(rankings[0]) + len(rankings[1]) == 0, \"Mistake: not ranking all docs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterleavingsStep(IRStep):\n",
    "\n",
    "    def __init__(self, name, purpose, data):\n",
    "        self.distribution = []\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "\n",
    "        saver = Saver(\"\", caching=CACHING_ON)\n",
    "\n",
    "        try:\n",
    "\n",
    "            return_dict = saver.load_python_obj(\"interleavings\")\n",
    "            return return_dict\n",
    "        except:\n",
    "\n",
    "\n",
    "            probabilistic_interleavings_list = []\n",
    "            team_draft_interleavings_list = []\n",
    "            self.distribution = softmax([norm.pdf(x, 0, 1.5) for x in range(3)])\n",
    "\n",
    "            for number, category in enumerate(input_list.values()):\n",
    "                print (\"\\nStart interleaving category {}\".format(number))\n",
    "\n",
    "                local_probabilistic_interleavings_list = []\n",
    "                local_team_draft_interleavings_list = []\n",
    "\n",
    "\n",
    "                for pair_number, (ranking1, ranking2) in enumerate(category):\n",
    "\n",
    "                    if ((pair_number % int(len(category)/10)) == 0):\n",
    "                        print(\"\\r{} out of {} done\".format(pair_number,  len(category)), end='')\n",
    "\n",
    "                    try:\n",
    "                        probabilistic_interleaving = ProbabilisticInterleaving(ranking1, ranking2, self.distribution)\n",
    "                        probabilistic_interleaving.cut_off_at(GAMMA_SIZE)\n",
    "                        local_probabilistic_interleavings_list.append(probabilistic_interleaving)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        draft_interleaving = TeamDraftInterleaving(ranking1, ranking2)\n",
    "                        draft_interleaving.cut_off_at(GAMMA_SIZE)\n",
    "                        local_team_draft_interleavings_list.append(draft_interleaving)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                probabilistic_interleavings_list.append(local_probabilistic_interleavings_list)\n",
    "                team_draft_interleavings_list.append(local_team_draft_interleavings_list)\n",
    "\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            return_dict = {\"probabilistic\": probabilistic_interleavings_list, \"team_draft\": team_draft_interleavings_list}\n",
    "            saver.save_python_obj(return_dict, \"interleavings\")\n",
    "            return return_dict\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"Finished step {}\".format(self.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 implementation:\n",
    "- Learning click model parameters for user-simulation using EM and ML\n",
    "- Defining click models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Click_Model(object):\n",
    "\n",
    "    def __init__(self, parameters, data):\n",
    "        self.parameters = parameters\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError(\"to be overrided\")\n",
    "\n",
    "    def apply(self, interleaving):\n",
    "        raise NotImplementedError(\"to be overrided\")\n",
    "\n",
    "    def get_sc(self):  # clicks for each session\n",
    "        \"\"\"\n",
    "        :param data - in the form of a list of libraries\n",
    "        :return - library where key = Session ID, value = query:[rank, url,click]\n",
    "        \"\"\"\n",
    "        sc = {}  # key = Session ID, value = (clicked rank, url)\n",
    "        current_q = {}\n",
    "        for i in self.data:\n",
    "            if i[\"SessionID\"] not in sc.keys():\n",
    "                sc[i['SessionID']] = {i['QueryID']: []}\n",
    "\n",
    "            if i['TypeOfAction'] == 'Q':\n",
    "                if i['QueryID'] not in sc[i[\"SessionID\"]].keys():\n",
    "                    sc[i['SessionID']][i['QueryID']] = []  # Empty if session does not result in click\n",
    "                current_q = {\"SessionID\": i[\"SessionID\"], \"QueryID\": i['QueryID'], \"ListOfURLs\": i[\"ListOfURLs\"]}\n",
    "                for r, u in enumerate(i[\"ListOfURLs\"]):\n",
    "                    sc[i['SessionID']][i['QueryID']].append([r + 1, u, False])\n",
    "            if i['TypeOfAction'] == 'C':\n",
    "                try:\n",
    "                    cur_q = current_q[\"ListOfURLs\"]\n",
    "                except:\n",
    "                    continue\n",
    "                for r, u in enumerate(cur_q):\n",
    "                    if u == i[\"URLID\"]:\n",
    "                        for sublist in sc[i['SessionID']][current_q['QueryID']]:\n",
    "                            if sublist[1] == u:\n",
    "                                sublist[2] = True\n",
    "        return sc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PBM(Click_Model):\n",
    "\n",
    "    def __init__(self, parameters, data):\n",
    "        super().__init__(parameters, data)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting training\")\n",
    "\n",
    "        uq = self.get_uq()  # change frame to whatever is the data saved as\n",
    "        sc = self.get_sc()\n",
    "        alphas = self.init_alphas(0.2)  # initializing first alpha\n",
    "        # gammas = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "        gammas = self.parameters\n",
    "        gs = [gammas]\n",
    "        als = [alphas]\n",
    "        convergence_e = 0.01\n",
    "        counter = 0\n",
    "\n",
    "        while 1 == 1:  # infinite loop\n",
    "            current_g = self.gamma_update(als[-1], gs[-1], uq, sc)\n",
    "            # current_g = gamma_update(alphas, gs[counter], uq, sc)\n",
    "            gs.append(current_g)\n",
    "\n",
    "            current_a = self.alpha_update(als[-1], gs[-1], uq, sc)\n",
    "            als.append(current_a)\n",
    "\n",
    "            if (len(als) > 3):\n",
    "                als.pop(0)\n",
    "\n",
    "            if (len(gs) > 3):\n",
    "                gs.pop(0)\n",
    "\n",
    "            counter += 1\n",
    "            print('\\rEM iteration number = {} Trained parameters = {}'.format(counter, current_g), end='')\n",
    "            if np.linalg.norm(np.array(gs[-1]) - np.array(gs[-2])) < convergence_e and counter > 0:  # Convergence criteria\n",
    "                print(\"\\n\")\n",
    "                self.parameters = current_g\n",
    "                return\n",
    "\n",
    "    def apply(self, interleaving):\n",
    "\n",
    "\n",
    "        interleaving.reset_score()\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        interleaving_list = interleaving.get_interleaved_ranking()\n",
    "\n",
    "        for index, document in enumerate(interleaving_list):\n",
    "            relevance = document.relevance_to_int()\n",
    "            if relevance == 1:\n",
    "                alpha = 1 - epsilon\n",
    "            else:\n",
    "                alpha = epsilon\n",
    "\n",
    "            dice = random.random()\n",
    "\n",
    "            evaluated = self.parameters[index] * alpha\n",
    "\n",
    "            if dice <= evaluated:\n",
    "                interleaving.insertclick(index)\n",
    "\n",
    "    def get_uq(self):\n",
    "        \"\"\"\n",
    "        :param data - in the form of a list of libraries\n",
    "        :return - library with key = document url, value = Query id: list of sessions\n",
    "        \"\"\"\n",
    "        uq = {}  # key = document url, value = Query id: list of sessions\n",
    "        for i in self.data:\n",
    "            if i['TypeOfAction'] == 'Q':\n",
    "                for u in i['ListOfURLs']:\n",
    "                    if u not in uq.keys():\n",
    "                        uq[u] = {i['QueryID']: [i['SessionID']]}  # add url and corresponding query and session id\n",
    "                    if u in uq.keys():\n",
    "                        if i['QueryID'] in uq[u].keys():  # check if document vs. query combo already exists\n",
    "                            uq[u][i['QueryID']].append(i['SessionID'])\n",
    "                        else:\n",
    "                            uq[u][i['QueryID']] = [i['SessionID']]\n",
    "        return uq\n",
    "\n",
    "\n",
    "\n",
    "    def init_alphas(self, value):\n",
    "        \"\"\"\n",
    "        :param data -  in the form of a list of libraries\n",
    "        :return - library where key = document, value = query : a_uq\n",
    "        \"\"\"\n",
    "        alphas = {}  # key = document, value = query : a_uq\n",
    "        for f in self.data:\n",
    "            if f['TypeOfAction'] == 'Q':\n",
    "                for u in f[\"ListOfURLs\"]:\n",
    "                    if u not in alphas.keys():\n",
    "                        alphas[u] = {f['QueryID']: value}\n",
    "                    if u in alphas.keys():\n",
    "                        if f['QueryID'] not in alphas[u].keys():\n",
    "                            alphas[u][f['QueryID']] = value\n",
    "        return alphas\n",
    "\n",
    "    def alpha_update(self, alphas, gammas, uq, sc):\n",
    "        \"\"\"\n",
    "        :param alphas - library where key = document, value = query : a_uq\n",
    "        :param gammas - list of 10 parameters\n",
    "        :param uq - library with key = document url, value = Query id: list of sessions\n",
    "        :param sc - library where key = Session ID, value = (clicked rank, url)\n",
    "        :return - update by iterating though all query vs. document seshs\n",
    "        \"\"\"\n",
    "        alpha2 = self.init_alphas(1)\n",
    "        rank = 1  # init rank\n",
    "        click = 0  # initialize click\n",
    "\n",
    "        for document in uq:\n",
    "            for query in uq[document]:\n",
    "                counter = 2\n",
    "\n",
    "                for session in uq[document][query]:\n",
    "                    counter += 1\n",
    "                    for e in sc[session][query]:\n",
    "                        if document == e[1]:\n",
    "                            rank = e[0]\n",
    "                            if e[2] == True:\n",
    "                                click = 1\n",
    "                            else:\n",
    "                                click = 0\n",
    "                            break\n",
    "\n",
    "                    if (click == 0):\n",
    "                        fraction = ((1 - gammas[rank - 1]) * alphas[document][query]) / (\n",
    "                                    1 - (gammas[rank - 1] * alphas[document][query]))  # check alphas[document][query]\n",
    "                        alpha2[document][query] += fraction\n",
    "                    else:\n",
    "                        alpha2[document][query] += 1\n",
    "\n",
    "                alpha2[document][query] /= counter\n",
    "\n",
    "                if alpha2[document][query] < 0:\n",
    "                    raise Exception\n",
    "\n",
    "                if alpha2[document][query] > 1:\n",
    "                    raise Exception\n",
    "        return alpha2\n",
    "\n",
    "    def gamma_update(self, alphas, gammas, uq, sc):\n",
    "        \"\"\"\n",
    "        :param alphas - library where key = document, value = query : a_uq\n",
    "        :param gammas - list of 10 parameters\n",
    "        :param uq - library with key = document url, value = Query id: list of sessions\n",
    "        :param sc - library where key = Session ID, value = (clicked rank, url)\n",
    "        :return - list of updated parameters\n",
    "        \"\"\"\n",
    "        s_r = {key+1 : 0 for key in range(len(gammas))}#, 7: 0, 8: 0, 9: 0, 10: 0}  # sessions per rank (counter)\n",
    "        # counter = 0\n",
    "        gamma = np.zeros(len(gammas))\n",
    "        rank = 1  # initialize rank\n",
    "        click = 0  # initialize click\n",
    "        for document in uq:\n",
    "            for query in uq[document]:\n",
    "                for session in uq[document][query]:\n",
    "                    for e in sc[session][query]:\n",
    "                        if document == e[1]:\n",
    "                            rank = e[0]\n",
    "                            s_r[rank] += 1\n",
    "                            if e[2] == True:\n",
    "                                click = 1\n",
    "                            else:\n",
    "                                click = 0\n",
    "                            break\n",
    "\n",
    "                    if (click == 0):\n",
    "                        fraction = (gammas[rank - 1] * (1 - alphas[document][query])) / (\n",
    "                                    1 - gammas[rank - 1] * alphas[document][query])\n",
    "\n",
    "                        gamma[rank - 1] += fraction\n",
    "                    else:\n",
    "                        gamma[rank - 1] += 1\n",
    "\n",
    "\n",
    "        for g in range(len(gamma)):\n",
    "            gamma[g] /= s_r[g + 1]\n",
    "\n",
    "        return list(np.around(gamma, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Random_Click_Model(Click_Model):\n",
    "\n",
    "    def __init__(self, parameters, data):\n",
    "        super().__init__(parameters, data)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting training\")\n",
    "\n",
    "        sc = self.get_sc()\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for session in sc:\n",
    "            for query in sc[session]:\n",
    "                for document in sc[session][query]:\n",
    "                    if document[2] is True:\n",
    "                        numerator += 1\n",
    "                    denominator += 1\n",
    "\n",
    "        rho = numerator / denominator\n",
    "\n",
    "        self.parameters = rho\n",
    "        print(\"Final rho parameter {}\".format(rho))\n",
    "        return\n",
    "\n",
    "\n",
    "    def apply(self, interleaving):\n",
    "        interleaving.reset_score()\n",
    "\n",
    "        interleaving_list = interleaving.get_interleaved_ranking()\n",
    "\n",
    "        for index, _ in enumerate(interleaving_list):\n",
    "            if random.random() <= self.parameters:\n",
    "                interleaving.insertclick(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserClicksSimulationStep(IRStep):\n",
    "\n",
    "    def __init__(self, name, purpose, data):\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "        length_interleaving = input_list[0]\n",
    "        save_and_load = Saver(\"\", caching=CACHING_ON)\n",
    "\n",
    "        pbm_model = PBM([0.2]*length_interleaving, self.data)\n",
    "        try:\n",
    "            print(\"Attempting loading gamma's from pickle\")\n",
    "            gammas_pbm = save_and_load.load_python_obj(\"gammas_pbm\")\n",
    "            pbm_model.parameters = gammas_pbm\n",
    "        except:\n",
    "            print(\"Did not find gamma's saved in pickle so will retrain and save\")\n",
    "            pbm_model.train()\n",
    "            save_and_load.save_python_obj(pbm_model.parameters, \"gammas_pbm\")\n",
    "\n",
    "        random_model = Random_Click_Model([0.1] * length_interleaving, self.data)\n",
    "        try:\n",
    "            print(\"Attempting loading rho's from pickle\")\n",
    "            rho_random = save_and_load.load_python_obj(\"rho_random\")\n",
    "            random_model.parameters = rho_random\n",
    "        except:\n",
    "            print(\"Did not find rho saved in pickle so will retrain and save\")\n",
    "            random_model.train()\n",
    "            save_and_load.save_python_obj(random_model.parameters, \"rho_random\")\n",
    "\n",
    "        return (pbm_model, random_model)\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"Finished step {}\".format(self.name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 implementation:\n",
    "- Running click simulations for online testing\n",
    "- Capturing experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Experiment(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, interleaving_interval_lists, click_model, name):\n",
    "        self.win_percentage = {}\n",
    "        self.interleaving_interval_lists = interleaving_interval_lists\n",
    "        self.click_model = click_model\n",
    "        self.name = name\n",
    "        self.k = 3000\n",
    "\n",
    "    def run(self):\n",
    "        self.win_percentage = initialize_err_table()\n",
    "        \n",
    "        logging = True\n",
    "\n",
    "        try:\n",
    "            f = open(\"Console_output_multiprocess_{}.txt\".format(self.name), \"w\")\n",
    "        except:\n",
    "            logging = False\n",
    "\n",
    "        # for each interval, for each ranking pair we first \n",
    "        # run interleaving model then the click model k times\n",
    "        for interval_index, interleaving_lists in enumerate(self.interleaving_interval_lists):\n",
    "\n",
    "            if (logging):\n",
    "                f.write(\"INTERVAL\" + str(interval_index) + \"\\n\")\n",
    "                f.flush()\n",
    "\n",
    "            self.win_percentage[interval_index] = []\n",
    "            for interleaving_index, interleaving in enumerate(interleaving_lists):\n",
    "\n",
    "                try:\n",
    "                    if (logging):\n",
    "                        f.write(\"#\" + str(interleaving_index) + \" out of {} in bin {}\\n\".format(str(len(interleaving_lists)), str(interval_index)))\n",
    "                        f.flush()\n",
    "\n",
    "                    wins = 0\n",
    "                    for _ in range(self.k):\n",
    "                        self.click_model.apply(interleaving)\n",
    "\n",
    "                        winner = interleaving.get_winner()\n",
    "                        if winner == \"E\":\n",
    "                            wins += 1\n",
    "\n",
    "                    current_pair_win_percentage = wins / self.k\n",
    "                    self.win_percentage[interval_index].append(current_pair_win_percentage)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        self.win_percentage[\"name\"] = self.name\n",
    "        if (logging):\n",
    "            f.flush()\n",
    "        return self.win_percentage, f, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterleavingSimulationStep(IRStep):\n",
    "    def __init__(self, name, purpose, data):\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "        probabilistic_interleavings_list = input_list[0][\"probabilistic\"]\n",
    "        team_draft_interleavings_list = input_list[0][\"team_draft\"]\n",
    "        probabilistic_click_model = input_list[1][\"probabilistic\"]\n",
    "        random_click_model = input_list[1][\"random\"]\n",
    "\n",
    "        experiment_1 = Experiment((probabilistic_interleavings_list), probabilistic_click_model, 1)\n",
    "        experiment_2 = Experiment((probabilistic_interleavings_list), random_click_model, 2)\n",
    "        experiment_3 = Experiment((team_draft_interleavings_list), probabilistic_click_model, 3)\n",
    "        experiment_4 = Experiment((team_draft_interleavings_list), random_click_model, 4)\n",
    "\n",
    "        save_and_load = Saver(\"\", caching=CACHING_ON)\n",
    "\n",
    "        experiments = [experiment_1, experiment_2, experiment_3, experiment_4]\n",
    "\n",
    "        ignores = []\n",
    "\n",
    "        q = mp.Queue()\n",
    "\n",
    "        processes = [mp.Process(target=self.experimenting, args=(exp, q)) for exp in experiments]\n",
    "\n",
    "        results = [None] * 4\n",
    "\n",
    "        try:\n",
    "            print(\"Running experiments: 1/4\")\n",
    "            result = save_and_load.load_python_obj(\"experiment1\")\n",
    "            results[0] = result\n",
    "            ignores.append(0)\n",
    "        except:\n",
    "            print(\"Started multiprocessing \" + str(1))\n",
    "            processes[0].start()\n",
    "\n",
    "        \n",
    "        try:\n",
    "            print(\"Running experiments: 2/4\")\n",
    "            result = save_and_load.load_python_obj(\"experiment2\")\n",
    "            results[1] = result\n",
    "            ignores.append(1)\n",
    "        except:\n",
    "            print(\"Started multiprocessing \" + str(2))\n",
    "            processes[1].start()\n",
    "\n",
    "        try:\n",
    "            print(\"Running experiments: 3/4\")\n",
    "            result = save_and_load.load_python_obj(\"experiment3\")\n",
    "            results[2] = result\n",
    "            ignores.append(2)\n",
    "        except:\n",
    "            print(\"Started multiprocessing \" + str(3))\n",
    "            processes[2].start()\n",
    "\n",
    "        try:\n",
    "            print(\"Running experiments: 4/4\")\n",
    "            result = save_and_load.load_python_obj(\"experiment4\")\n",
    "            results[3] = result\n",
    "            ignores.append(3)\n",
    "        except:\n",
    "            print(\"Started multiprocessing \" + str(4))\n",
    "            processes[3].start()\n",
    "            \n",
    "            \n",
    "        print(\"Suggestion: in terminal enter 'tail -1000f [LOGFILE]' to follow progress of this part\")\n",
    "\n",
    "        for experiment_index in range(4):\n",
    "            if (experiment_index in ignores):\n",
    "                continue\n",
    "                \n",
    "            print(\"Waiting for result experiment, please be patient\\nSee logfiles of multiprocesses 'Console_output_multiprocess_#.txt'(only available if filewriting is enabled)\\n\")\n",
    "            result = q.get()\n",
    "            index = result[\"name\"]\n",
    "            results[index-1] = result\n",
    "            del result[\"name\"]\n",
    "            print(\"Got result back from experiment {}\\n\".format(index))\n",
    "\n",
    "        for i, p in enumerate(processes):\n",
    "            if (i in ignores):\n",
    "                continue\n",
    "            print(\"Attempting joining back in multiprocess\\n\")\n",
    "            p.join()\n",
    "            print(\"Joined experiment {} back in sucessfully\\n\".format(i+1))\n",
    "\n",
    "\n",
    "        for i, res in zip([1,2,3,4], results):\n",
    "            save_and_load.save_python_obj(res, \"experiment{}\".format(i))\n",
    "\n",
    "        result_1, result_2, result_3, result_4 = results[0], results[1], results[2], results[3]\n",
    "\n",
    "        print(\"\\rRunning experiments: Done!\")\n",
    "\n",
    "        result = {\"pbm\" : {\"probabilistic_interleaving\" : result_1, \"team_draft\" : result_3}, \"random\" : {\"probabilistic_interleaving\" : result_2, \"team_draft\" : result_4}}\n",
    "\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"Finished step {}\".format(self.name))\n",
    "\n",
    "    def experimenting(self, experiment, q):\n",
    "        result, file, logging = experiment.run()\n",
    "        \n",
    "        if (logging):\n",
    "            file.write(\"done experiment\\n\")\n",
    "            file.flush()\n",
    "        \n",
    "        q.put(result)\n",
    "        \n",
    "        if (logging):\n",
    "            file.write(\"put data in queue\\n\")\n",
    "            file.flush()\n",
    "            file.close()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 implementation:\n",
    "- Interpreting results\n",
    "- Calculating final sample size required for each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleSizeStep(IRStep):\n",
    "\n",
    "    def __init__(self, name, purpose, data):\n",
    "        self.alpha = 0.05\n",
    "        self.beta = 0.1\n",
    "        self.p_0 = 0.5\n",
    "        super().__init__(name, purpose, data)\n",
    "\n",
    "\n",
    "    def onStart(self, input_list):\n",
    "\n",
    "        total_table = {}\n",
    "\n",
    "        for cm_index, click_model in enumerate(input_list):\n",
    "\n",
    "            total_table[click_model] = {}\n",
    "\n",
    "            for it_index, interleaving_type in enumerate(input_list[click_model]):\n",
    "                current_index = (cm_index + 1) + (it_index + 1)\n",
    "                print(f'\\rCalculating: {current_index}/4', end='')\n",
    "\n",
    "                total_table[click_model][interleaving_type] = {}\n",
    "\n",
    "                for bin in input_list[click_model][interleaving_type]:\n",
    "\n",
    "                    str_bin = str(bin)\n",
    "\n",
    "                    total_table[click_model][interleaving_type][str_bin] = []\n",
    "                    current_bin = []\n",
    "\n",
    "                    for percentage in input_list[click_model][interleaving_type][bin]:\n",
    "\n",
    "                        proportion_test = self.proportion_test(percentage, self.alpha, self.beta, self.p_0)\n",
    "                        if not proportion_test:\n",
    "                            continue\n",
    "\n",
    "                        current_bin.append(proportion_test)\n",
    "\n",
    "\n",
    "                    max = \"None\"\n",
    "                    min = \"None\"\n",
    "                    median = \"None\"\n",
    "                    mean = \"None\"\n",
    "                    std = \"None\"\n",
    "\n",
    "                    if (len(current_bin) > 0):\n",
    "\n",
    "                        max_chunks = 250\n",
    "                        current_bin = average_chunks(current_bin, max_chunks)\n",
    "                        \n",
    "                        max = np.max(current_bin)\n",
    "                        min = np.min(current_bin)\n",
    "                        median = np.median(current_bin)\n",
    "                        mean = np.mean(current_bin)\n",
    "                        std = np.std(current_bin)\n",
    "\n",
    "                    total_table[click_model][interleaving_type][str_bin] = { \n",
    "                        \"max\" : max, \n",
    "                        \"min\" : min, \n",
    "                        \"median\": median, \n",
    "                        \"mean\" : mean, \n",
    "                        \"std\": std,\n",
    "                        \"list\" : current_bin}\n",
    "\n",
    "        print('\\rCalculating: Done!')\n",
    "        return total_table\n",
    "\n",
    "    @lru_cache(maxsize=320000)\n",
    "    def n(self, alpha, beta, p_0, p_1):\n",
    "\n",
    "        z = (p_1-p_0)/(math.sqrt((p_0*(1-p_0)/self.k)))\n",
    "\n",
    "        nominator = (\n",
    "                    (z-alpha*math.sqrt(p_0*(1-p_0)))\n",
    "                     +\n",
    "                     (z-beta*math.sqrt(p_1*(1-p_1)))\n",
    "                    )\n",
    "\n",
    "        denominator = abs(p_1-p_0)\n",
    "\n",
    "        if (denominator == 0):\n",
    "            # instead of inf for printing reasons\n",
    "            return 9999999999999999\n",
    "\n",
    "        return round(float(nominator/denominator)**2 + float(1/denominator))\n",
    "\n",
    "    @lru_cache(maxsize=320000)\n",
    "    def proportion_test(self, p1, alpha = 0.5, beta = 0.1, p0 = 0.5):\n",
    "        z_alpha = scipy.stats.norm.ppf(1-alpha)\n",
    "        z_beta = scipy.stats.norm.ppf(1-beta)\n",
    "        if p1 == p0:\n",
    "            return None\n",
    "\n",
    "        diff = p1 - p0\n",
    "        sample_size = p0 * (1 - p0) * (z_alpha + z_beta * np.sqrt((p1 * (1-p1))/(p0 *(1-p0)))/(diff))**2\n",
    "        if sample_size==math.inf:\n",
    "            print('INF:', p0, p1, z_alpha, z_beta)\n",
    "\n",
    "        return sample_size\n",
    "\n",
    "    def onfinish(self):\n",
    "        print(\"Finished step {}\".format(self.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together:\n",
    "- Calling all steps\n",
    "- Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    \n",
    "    save_and_load = Saver(\"\", caching=CACHING_ON)\n",
    "\n",
    "    ## Step 0 : Loading data\n",
    "\n",
    "    data = save_and_load.load_data_model_1()\n",
    "\n",
    "    steps = [\n",
    "        RankingsStep(1, \"Simulate Rankings of Relevance for E and P\", data),\n",
    "        ERRStep(2, \"Calculate the 𝛥measure\", data),\n",
    "        InterleavingsStep(3, \"Implement Team-Draft Interleaving and Probabilistic Interleaving \", data),\n",
    "        UserClicksSimulationStep(4, \"Simulate User Clicks\", data),\n",
    "        InterleavingSimulationStep(5, \"Simulate Interleaving Experiment\", data),\n",
    "        SampleSizeStep(6, \"Compute Sample Size\", data),\n",
    "    ]\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    def do_next_step(input_list, counter):\n",
    "        step_output = steps[counter].do_step(input_list)\n",
    "        counter += 1\n",
    "        return step_output, counter\n",
    "\n",
    "    ## Step 1: Simulate Rankings of Relevance for E and P\n",
    "\n",
    "    rankings_pairs, counter = do_next_step([int(DATABASE_SIZE/2)], counter)\n",
    "\n",
    "    ## Step 2: Calculate the 𝛥measure\n",
    "\n",
    "    err_table, counter = do_next_step(rankings_pairs, counter)\n",
    "\n",
    "    ## Step 3: Implement Team-Draft Interleaving (5pts) and Probabilistic Interleaving (35 points)\n",
    "\n",
    "    interleaving_dictionary, counter = do_next_step(err_table, counter)\n",
    "\n",
    "    ## Step 4: Simulate User Clicks (40 points)\n",
    "\n",
    "    click_models, counter = do_next_step([GAMMA_SIZE], counter)\n",
    "\n",
    "    ## Step 5: Simulate Interleaving Experiment\n",
    "\n",
    "    resulting_dictionary, counter = do_next_step([interleaving_dictionary, {\"probabilistic\": click_models[0], \"random\": click_models[1]}], counter)\n",
    "    \n",
    "    ## Step 6: Compute Sample Size\n",
    "\n",
    "    filled_in_table, counter = do_next_step(resulting_dictionary, counter)\n",
    "\n",
    "    print(\"#######\\n\\n\\nDONE\\n\\n\\n\")\n",
    "\n",
    "    save_and_load.save_python_obj(filled_in_table, \"Final_result\")\n",
    "    \n",
    "    return filled_in_table, resulting_dictionary, pr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING CSV\n",
    "\n",
    "def average_experiment_dict(experiment_dict):\n",
    "    max_chunks = 250\n",
    "    for i in range(10):\n",
    "        if len(experiment_dict[i]) > 0:\n",
    "            experiment_dict[i] = average_chunks(experiment_dict[i], max_chunks)\n",
    "\n",
    "    return experiment_dict\n",
    "\n",
    "def csv_ing_it(final_, percentages):\n",
    "    \n",
    "    print(\"Starting csv export\")\n",
    "    \n",
    "    saver = Saver(\"\", caching=CACHING_ON)\n",
    "\n",
    "    try:\n",
    "        final = saver.load_python_obj(\"Final_result\")\n",
    "    except:\n",
    "        final = final_\n",
    "\n",
    "    try:\n",
    "        percentages1 = average_experiment_dict(saver.load_python_obj(\"experiment1\"))\n",
    "        percentages2 = average_experiment_dict(saver.load_python_obj(\"experiment2\"))\n",
    "        percentages3 = average_experiment_dict(saver.load_python_obj(\"experiment3\"))\n",
    "        percentages4 = average_experiment_dict(saver.load_python_obj(\"experiment4\"))\n",
    "\n",
    "        result = {\"pbm\" : {\"probabilistic_interleaving\" : percentages1, \"team_draft\" : percentages3}, \"random\" : {\"probabilistic_interleaving\" : percentages2, \"team_draft\" : percentages4}}\n",
    "    except:\n",
    "        result = percentages\n",
    "        \n",
    "    output_file_1 = open(\"impressions.csv\", \"w\")\n",
    "\n",
    "    for click_model in final:\n",
    "        for interleaving in final[click_model]:\n",
    "\n",
    "            output_file_1.write(\"{},-,{},\\n\".format(click_model, interleaving))\n",
    "            output_file_1.write(\"\\n\")\n",
    "            output_file_1.write(\"bin,max,min,median,stdev,mean,whole_list,\\n\")\n",
    "\n",
    "            for bin in final[click_model][interleaving]:\n",
    "\n",
    "                listbuilder = \"\"\n",
    "                for element in final[click_model][interleaving][bin][\"list\"]:\n",
    "                    listbuilder = listbuilder + str(element) + \",\"\n",
    "\n",
    "                max = final[click_model][interleaving][bin][\"max\"]\n",
    "                min = final[click_model][interleaving][bin][\"min\"]\n",
    "                median = final[click_model][interleaving][bin][\"median\"]\n",
    "                mean = final[click_model][interleaving][bin][\"mean\"]\n",
    "                stdev = final[click_model][interleaving][bin][\"std\"]\n",
    "\n",
    "                bin_output = \"{},{},{},{},{},{},{},\\n\".format(bin, max, min, median, stdev, mean, listbuilder)\n",
    "                output_file_1.write(bin_output)\n",
    "\n",
    "            output_file_1.write(\"\\n\")\n",
    "            output_file_1.write(\"\\n\")\n",
    "\n",
    "\n",
    "    output_file_1.close()\n",
    "\n",
    "    output_file_2 = open(\"wins.csv\", \"w\")\n",
    "\n",
    "    for click_model in result:\n",
    "        for interleaving in result[click_model]:\n",
    "\n",
    "            output_file_2.write(\"{},-,{},\\n\".format(click_model, interleaving))\n",
    "            output_file_2.write(\"\\n\")\n",
    "            output_file_2.write(\"bin,whole_list,\\n\")\n",
    "\n",
    "            for bin in result[click_model][interleaving]:\n",
    "\n",
    "                listbuilder = \"\"\n",
    "                for element in result[click_model][interleaving][bin]:\n",
    "                    listbuilder = listbuilder + str(element) + \",\"\n",
    "\n",
    "                bin_output = \"{},{},\\n\".format(bin, listbuilder)\n",
    "\n",
    "                output_file_2.write(bin_output)\n",
    "\n",
    "            output_file_2.write(\"\\n\")\n",
    "            output_file_2.write(\"\\n\")\n",
    "\n",
    "    output_file_2.close()\n",
    "    \n",
    "    print(\"Finished csv export\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    results, percentages, pr  = main()\n",
    "    try:\n",
    "        csv_ing_it(results, percentages)\n",
    "    except:\n",
    "        print(\"csv file writing failed\")\n",
    "    \n",
    "    print(\"Some performance statistics:\")\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = 'cumulative'\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    print (s.getvalue())   # COMMENT OUT TO IGNORE TIME PERFORMANCE\n",
    "    print(\"\\n\\n\\nSee 'wins.csv' & 'impressions.csv' for results of experiments for 'the percentage of wins' in the experiment of algorithm E and its resulting 'needed impressions for statistical significance' respectively\\n\\n\\n\")\n",
    "    \n",
    "    try:# removing log files\n",
    "        for x in range(1,5):\n",
    "            os.system(\"rm Console_output_multiprocess_{}.txt\".format(x))\n",
    "        print(\"Sucessfully deleted logfiles\")\n",
    "    except:\n",
    "        print(\"Failed to remove logfiles, you might wanna look at that yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- INSERT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
